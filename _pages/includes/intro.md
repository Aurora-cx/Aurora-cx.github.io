<span class='anchor' id='about-me'></span>
Hi! I'm Chenxi Wang (Aurora), a second-year M.Sc. student in NLP at [MBZUAI](https://mbzuai.ac.ae/), supervised by [Prof. Xiuying Chen](https://iriscxy.github.io/). Prior to MBZUAI, I got my B.Eng. degree in Computer Science at [Xi'an Jiaotong University](https://en.xjtu.edu.cn/).


<!-- My research focuses on human-centered AI. I take an **interpretability-first post-training** approach to **“awaken”** latent knowledge and behaviors in pretrained LLMs that are not actively invoked under normal inference. I use interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, **without additional large-scale pretraining**. -->
<!-- My research focuses on human-centered AI. Pretrained LLMs encode rich knowledge and behaviors that normal inference does not actively invoke. I take an **interpretability-first post-training** approach to **“awaken”** these latent capabilities and make them reliably available on demand. Using interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, I identify and modulate the internal mechanisms that give rise to specific behaviors. This enables controlled activation of these capabilities at inference, without additional large-scale pretraining. -->
<!-- My research focuses on uncovering the latent capabilities of large language models (LLMs). These models encode far richer knowledge and behaviors than they naturally express under normal inference, and these capabilities do not spontaneously manifest. I take an **interpretability-first post-training** approach to **"awaken"** these latent capabilities and make them reliably available on demand. Using interpretability techniques such as activation patching, subspace analysis, steering, and circuit discovery and control, I localize and modulate the internal mechanisms that give rise to specific behaviors. This enables controlled, inference-time activation of these capabilities **without additional large-scale pretraining and even supports real-time adaptation to user needs**. -->
My research focuses on uncovering the latent capabilities of large language models (LLMs). These models encode far richer knowledge and behaviors than they naturally express under normal inference; however, these capabilities do not spontaneously manifest. I take an **interpretability-first post-training** approach to **awaken** these latent capabilities and make them reliably available on demand. Using interpretability techniques such as circuit discovery and control, I localize and modulate the internal mechanisms that give rise to specific behaviors. This enables controlled, inference-time activation of these capabilities without additional large-scale pretraining, paving the way for more adaptive and efficient model behaviors.

<mark style="background-color: #FFE4E1; padding: 2px 4px;">🌟 My current work explores <strong>AI that meets human emotional needs</strong>, building explainable, controllable, and personalized emotional-support systems that can evolve with user needs.</mark>

I welcome thoughtful discussions on dependency risk, and I’m happy to chat about philosophy of mind and what makes AI feel more human :)

<p style="color: #FF6B35; font-weight: bold;">Actively seeking PhD positions for Fall 2026. Please see my CV and feel free to get in touch.</p>

