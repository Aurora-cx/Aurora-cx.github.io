<span class='anchor' id='about-me'></span>
Hi! I'm Chenxi Wang (Aurora), a second-year M.Sc. student in NLP at MBZUAI, supervised by [Prof. Xiuying Chen](https://iriscxy.github.io/). I earned my B.Eng. in Computer Science from Xi'an Jiaotong University, where my undergraduate thesis ranked first in my cohort.

<!-- My research focuses on human-centered AI. I take an **interpretability-first post-training** approach to **“awaken”** latent knowledge and behaviors in pretrained LLMs that are not actively invoked under normal inference. I use interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, **without additional large-scale pretraining**. -->
<!-- My research focuses on human-centered AI. Pretrained LLMs encode rich knowledge and behaviors that normal inference does not actively invoke. I take an **interpretability-first post-training** approach to **“awaken”** these latent capabilities and make them reliably available on demand. Using interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, I identify and modulate the internal mechanisms that give rise to specific behaviors. This enables controlled activation of these capabilities at inference, without additional large-scale pretraining. -->
My research focuses on uncovering the latent capabilities of large language models (LLMs). These models encode far richer knowledge and behaviors than they naturally express under normal inference, and these capabilities do not spontaneously manifest. I take an **interpretability-first post-training** approach to **"awaken"** these latent capabilities and make them reliably available on demand. Using interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, I localize and modulate the internal mechanisms that give rise to specific behaviors. This enables controlled, inference-time activation of these capabilities **without additional large-scale pretraining and even supports real-time adaptation to user needs**.

<mark style="background-color: #FFE4E1; padding: 2px 4px;">🌟 My current work explores <strong>AI that meets human emotional needs</strong>, building explainable, controllable, and personalized emotional-support AI that can evolve with user needs.</mark>

I welcome thoughtful discussions on dependency risk, and I’m happy to chat about philosophy of mind and what makes AI feel more human :)

<p style="color: #FF6B35; font-weight: bold;">Actively seeking PhD positions for Fall 2026. Please see my CV and feel free to get in touch.</p>

