<span class='anchor' id='about-me'></span>
Hi! I'm Chenxi Wang (Aurora), a second-year M.Sc. student in NLP at MBZUAI, supervised by [Prof. Xiuying Chen](https://iriscxy.github.io/). I earned my B.Eng. in Computer Science from Xi'an Jiaotong University, where my undergraduate thesis ranked first in my cohort.

<!-- My research focuses on human-centered AI. I take an **interpretability-first post-training** approach to **‚Äúawaken‚Äù** latent knowledge and behaviors in pretrained LLMs that are not actively invoked under normal inference. I use interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, **without additional large-scale pretraining**. -->
My research focuses on human-centered AI. Pretrained LLMs encode rich knowledge and behaviors that normal inference does not actively invoke. I take an **interpretability-first post-training** approach to **‚Äúawaken‚Äù** these latent capabilities and make them reliably available on demand. Using interpretability techniques such as circuit discovery and control, subspace analysis, steering vectors, activation patching, and sparse autoencoders, I identify and modulate the internal mechanisms that give rise to specific behaviors. This enables controlled activation of these capabilities at inference, without additional large-scale pretraining.

üåü <mark style="background-color: #FFE4E1; padding: 2px 4px;">My current work explores **AI that meets human emotional needs**, building explainable, controllable, and personalized emotional-support AI that can evolve with user needs.</mark>

Welcome thoughtful discussions on dependency risk, and I‚Äôm happy to chat about philosophy of mind and what makes AI feel more human :)

<p style="color: #FF6B35; font-weight: bold;">Actively seeking PhD positions for Fall 2026. Please see my CV and feel free to get in touch.</p>

